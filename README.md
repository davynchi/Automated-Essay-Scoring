Пайплайн для обучения и инференса модели для автоматического выставления оценок
по написанным эссе.

# Automated Essay Scoring

#### Давыдов Михаил

## Постановка задачи

Проблема -- учителя тратят слишком много времени на проверку эссе студентов и
школьников. Цель -- создать систему автоматического выставления оценок для эссе
школьников с помощью машинного обучения.

### Формат входных и выходных данных

Формат входных данных -- таблица с 2 колонками:

1. `essay_id` - уникальный id для эссе.
2. `full_text` - полный текст эссе.

Формат выходных данных -- таблица `submission.csv` с 2 колонками:

1. `essay_id` - уникальный id для эссе. Все id совпадают с таковыми во входных
   данных.
2. `score` - честная оценка для эссе по шкале натуральных чисел от 1 до 6.

### Метрики

Ответы оцениваются по метрике `quadratic weighted kappa`, показывающая степень
согласия между двумя оценками. Обычно метрика меняется от 0 (случайное согласие)
до 1 (полное согласие). Если степень согласия между метриками меньше, чем для
случайного, то оценка может уйти ниже 0.

`quadratic weighted kappa` вычисляется так: Сначала создается матрица $O$
размера $N \times N$, причем $O_{i,j}$ отвечает за количество эссе с изначальной
оценкой $i$ и получивших предсказание оценки $j$. Матрица весов $w$ размера
$N \times N$ создается по формуле:

$$
w_{i, j} = \frac{(i − j)^2}{(N − 1)^2}
$$

Матрица $E$ размера $N \times N$ вычисляется так: сначала создается вектора $u$
и $v$, у первого на $i$-ом месте стоит количество оценок $i$ в тестовой выборке,
$v$ -- так же, но для предсказанных значений. Далее $E = u \otimes v$,
нормализованное так, чтобы суммы элементов у $E$ и $O$ были одинаковыми.

`quadratic weighted kappa` из этих матриц вычисляется как:

$$
\kappa = 1 - \frac{\sum_{i, j}w_{i,j}O_{i,j}}{\sum_{i, j}w_{i,j}E_{i,j}}
$$

Исходные оценки ставились по
[этим критериям](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf).

### Валидация

train и test выборки разделены изначально. Сделано, так как seed на разных ОС
может выдавать разные результаты, и деление на train и test может на них
отличаться. На инференсе результаты прогона на `test.csv` сравниваются с
реальными оценками на текстах из `test.csv`, и вычисляется итоговая метрика.

Стратегия во время обучения: мы пользуемся Multilabel Stratified K‑Fold из
пакета `iterstrat.ml_stratifiers`. Стратификация идёт сразу по двум
признакам‑меткам: `prompt_name` (тематика эссе) + `score` (итоговый балл). Для
тех эссе, которые не присутствуют в датасете `persuade...`, то есть у них нет
`prompt_name`, считаем , что уних пустой `prompt_name`. Благодаря этому каждый
из K фолдов отражает исходное распределение тем и оценок.

```python
mskf = MultilabelStratifiedKFold(
        n_splits=cfg.n_folds,
        shuffle=True,
        random_state=cfg.seed)
```

Валидация на этапах:

1. Finetune:

   Выделяем отдельно фолд под номером 0. Учимся на всех остальных фолдах,
   валидируемся на фолде 0.

2. Train:

   Для каждого фолда создается своя модель, и для нее происходит двойная
   валидация. Все данные делятся на два типа: те, у которых есть `prompt_name`
   (Data A, `flag == 0`), и те, у которых нет (Data B, `flag == 1`). Обучение
   происходит в 2 этапа:

   1. Обучаемся на $fold \neq i$, валидируемся на $fold = i \land flag = 1$.
   2. Обучаемся на $fold \neq i \land flag = 1$, валидируемся на
      $fold = i \land flag = 0$.

   Таким образом, валидационная выборка я первого этапа не протекает во второй
   этап, особое внимание уделяется новвым данным (то есть данным без промтов).
   При этом не происходит переобучения на Data B.

   Кроме того, в mlflow сохраняется значение метрики еще и на валидационной
   выборке $fold = i$ (то есть любой флаг). Так мы можем проверять, насколько
   результаты с вал. выьорки соотносятся с результатами от более общей выборки.

### Данные

На входе для тренировки в папке `data/raw/train` представлены файлы:

- `train.csv` - эссе и их оценки. 3 колонки:
  1. `essay_id` - уникальный id для эссе.
  2. `full_text` - полный текст эссе.
  3. `score` - честная оценка для эссе по шкале от 1 до 6.
- `persuade_2.0_human_scores_demo_id_github.csv` -- датасет публичных эссе.
  Большая часть текстов (примерно $\frac{2}{3}$), которая используется для
  обучения, взята из
  [публичного датасета](https://www.kaggle.com/datasets/garried/persuade-2-0). В
  таблице много разных столбцов, но важны только два:
  1. `full_text` - полный текст эссе. Аналогичен полю `full_text` у `train.csv`
     и `test.csv`.
  2. `prompt_name` -- тема эссе. Всего в этом датасете 6 различных тем эссе, эти
     темы используются для качественного деления данных для валидации.

Для тестирования в папке `data/raw/test` представлены файлы:

- `test.csv` - эссе, использумеые как тестовые данные. Такие же поля, как в
  train.csv, но без поля `score`.
- `test_score.csv` - оценки для текстов из `test.csv` и только для них.
  Представлены колонкой `score`. Используется для сравнения итоговых оценок на
  тесте с ground truth.

Кроме того, в папке `data/raw/additional` представлены файлы:

- `sample_submission.csv` - пример файла для сдачи в корректном формате. 2
  колонки:
  1. `essay_id` - уникальный id для эссе
  2. `score` - предсказанная честная оценка для эссе по шкале от 1 до 6
- `full_train.csv` - полный датасет текстов и оценок, то есть объединение
  датасетов `train.csv` и `test.csv`.

Исходные данные (таблицы `train.csv`[= `full_train.csv`], `test.csv`,
`sample_submission.csv`), можно найти
[в разделе "Data"](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/data)
соревнования kaggle.

## Моделирование

### Бейзлайн

Бейзлайн -- взять языковую модель, подать на вход
[критерий оценивания](https://storage.googleapis.com/kaggle-forum-message-attachments/2733927/20538/Rubric_%20Holistic%20Essay%20Scoring.pdf)
в качестве промта и попросить оценить для поданного текста из тестовой выборки
итоговую оценку. То есть не используем обучающую выборку совсем.

### Основная модель

| Этап                        | Что делаем                                                                                                                                                                                                                                                                                                                                                | Ссылки / детали                                                                    |
| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| Файнтюн (MLM)               | Берём пред-тренированный DeBERTa v3 base/large и доучиваем его на «сырых» текстах задач датасета с задачей Masked-LM; длина блока — `block_size` в конфиге.                                                                                                                                                                                               | папка `finetune`                                                                   |
| Stage 1 (A + B, монитор B)  | Модель есть конкатенация двух архитектур - _Backbone_ и _Head_<br> _Backbone_ — зафиксированный чекпоинт из шага MLM;<br>_Head_ — один из трёх вариантов:<br> – `mean_pooling` (нейтральный baseline)<br> – `attention` (взвешенная сумма)<br> – `lstm` (двухслойная Bi-LSTM поверх токенов).<br>_Loss_ — `BCEWithLogitsLoss`, т.к. регрессируем в [0,1]. | `train/train_lightning.py`,<br> `model`,<br> `dataset`                             |
| Stage 2 (B-only, монитор A) | Инициализируем лучшим чекпоинтом с предыдущего шага, тренируем только на флагах 1 и валидируем на A, чтобы не переобучиться.<br>Target-smoothing: `score_s = (1 - sl_rate) * score / 5 + sl_rate * oof_pred`.                                                                                                                                             | функция `load_pickle_data` в `train/train_lightning.py`                            |
| Оптимизация                 | `AdamW`, весовой decay для ненормализуемых параметров; planner — linear или cosine с warm-up, шаг — каждый батч.<br>AMP (`precision=16`) на GPU.                                                                                                                                                                                                          | `configure_optimizers()` в классе `EssayScoringPL` из `model.lightning_modules.py` |
| Датасет                     | В классе `EssayDataModule` деление на обучающую и валидационные выборки. Под капотом этого класса хрнаится класс `LALDataset`, выдающие тексты в батче, обрезанные через `collate` до наибольшей длины текста в батче.                                                                                                                                    | папка `dataset`.                                                                   |
| Конвертация модели          | Используем форматы `onnx` и `TensorRT` для конвертации каждой модели и складывания ее вместе с конфигом в папку для `triton_repo` для инференса.                                                                                                                                                                                                          | `train/prepare_to_infer.py`                                                        |
| Ensemble / blending         | K фолдов × N моделей одного типа = N · K моделей.<br>OOF-предсказания сохраняются, веса ансамбля подбираются через алгоритм Nelder–Mead (scipy) с оптимизацией Quadratic Weighted Kappa (QWK).                                                                                                                                                            | `train/calc_ensemble_weights.py`                                                   |
| Инференс                    | Реализован через Triton Inference Server.<br>Предсказываем логит для каждой модели (определяемая фолдом и базовой архитектурой моедли), усредненяем по фолдам, берем взвешенную сумму по моделям.                                                                                                                                                         | папка `inference`                                                                  |

- DeBERTa v3 base — [microsoft/deberta‑v3‑base](https://huggingface.co/microsoft/deberta-v3-base)
- DeBERTa v3 large — [microsoft/deberta‑v3‑large](https://huggingface.co/microsoft/deberta-v3-large)
- Multilabel
  Stratified K‑Fold — https://github.com/trent-b/iterative-stratification

## Внедрение

Модель можно будет запускать для оценки имеющихся текстов. Для этого все
чекпоинты модели и все веса модели должны находиться на Triton сервере, из
которого будет запускаться модель для инференса. В идеале этой моделью иогут
пользоваться все учителя для ускоренной проверки эссе.

## Техническая часть

### Setup

Для скачивания проекта:

`git clone git@github.com:davynchi/Automated-Essay-Scoring.git`

Далее:

```bash
conda create --name your_conda_env_name
conda activate your_conda_env_name
cd Automated-Essay-Scoring
conda install -c conda-forge poetry
poetry config virtualenvs.create false --local
poetry install
pre-commit install
```

Если нет Docker: `sudo snap install docker`

Ограничить набор GPU для тренировки можно через
`export CUDA_VISIBLE_DEVICES=your_cuda_device_num`

### Train

Для запуска кода для тренировки в терминале написать:

```bash
python -m automated_essay_scoring.do_train
```

Параметры:

- `skip_preprocessing: bool = False`

  Если `True`, то пропускаем этап проепроцессинга; в таком случае эти модели
  должны лежать по пути `CACHED_DATA_PATH = "./cached_data"`.

- `skip_pretrain_phase: bool = False`

  Если `True`, то пропускаем этап файнтюна модели; в таком случае эти модели
  должны лежать по пути `OUTPUT_DIR_FINETUNED="./pretrained_models"`.

- `skip_train_phase: bool = False`

  Если `True`, то пропускаем этап обучения модели; в таком случае обученные
  модели лежать `OUTPUT_DIR_TRAIN = "./trained_models"`.

- `skip_best_ensemble : bool = False`

  Если `True`, то пропускаем этап поиска оптимальных весов ансамбля
  (использовать `best_ensemble_weights.npy`, если он уже существует).

- `skip_converting_to_tensorrt: bool = False`

  Если `True`, то не конвертируем модели в формат `TensorRT`, вместо этого будет
  использоваться формат `onnx`.

Соответственно, обучение состоит из 5 этапов:

1. Загрузка данных. Данные загружаются из удаленного s3-хранилища, если данных с
   такими названиями нет локально (чтобы не загружать лишний раз).
2. Препроцессинг. На этом этапе загружаются данные из s3-хранилища (если их
   нет). Регистрируются новые функции обработки текста. Добавляются колонки
   `fold`, `flag`. Они участвуют в формировании тренировочного и валидационного
   датасетов на этапе файнтюна и самой тренировки.
3. Обучение. Уже подробно описано в разделах "Основная модель" и "Валидация".
4. Конвертация модели в формат для инференса. Описано в строке "Конвертация
   модели" в разделе "Основная модель".
5. Подбор весов для ансамбля моделей. Описано в строке "Ensemble/blending" в
   разделе "Основная модель".

### Production preparation

Каждая модель (фолд и архитектура) переводится в формат `onnx`. Далее каждая из
моделей при `skip_converting_to_tensorrt = False` переводится из `onnx` в формат
`TensorRT`. Каждая такая модель в новом формате (`TensorRT` или `onnx`, если
пропустили конвертацию в `TensorRT`) складывается в соответствующую папку вместе
с triton-конфигом внутри папки `triton_repo`.

### Infer

После окончания работы `do_train` запустить:

```bash
sudo docker run \
  --runtime=nvidia --rm -it \
  -p 8000:8000 -p 8001:8001 -p 8002:8002 \
  -v "$HOME/triton_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 tritonserver \
  --model-repository=/models
```

Запустится Triton Inference Server на всех GPU. Если хочется запустить только на
определенной GPU, дполнительно прописать в команду:
`-e CUDA_VISIBLE_DEVICES=3 \`

Далее для запуска инференса прописать:

```bash
python -m automated_essay_scoring.do_infer
```

Параметров у модуля нет.

Итоговый файл с оценками будет лежать по пути
`SUBMISSION_PATH = "./data/submission/submission.csv"`

## План реализации проекта

- [x] Выбрать проект
- [x] Разобраться с conda и poetry
- [x] Разделить проект по функциям
- [x] Добавить средства поддержки quality tools
- [x] Научить запускать проект как локально, так и в Kaggle
- [x] Заставить проект работать
- [x] Первичный рефакторинг
- [x] Отдебажить конфигурации с гидрой
- [x] Разобраться с GroupKFold
- [x] Убрать все ворнинги
- [x] Отрефакторить submission
- [x] Добавить MLFlow
- [x] Дописать readme.md
- [x] Изменить поведение логгера
- [x] Добавить Lightning, add mlflow again
- [x] Добавить возможность контроля выбранных частей кода (без файнтюна или
      только инференс) через Fire.
- [x] Выделить расчет лучших весов в отдельный код и дать возможность запускать
      с уже имеющимися весами.
- [x] Добавить аннотации типов
- [x] Написать документацию
- [x] Еще один рефакторинг -- пересмотр своего кода
- [x] Добавить dvc
- [x] Добавить сохранение и подгрузку в формате onnx
- [x] Переместить и переименовать папку conf
- [x] Добавить сохранение моделей через TensorRT (или TorchScript)
- [x] Добавить Triton для инференса
- [x] Переписать Project Description в README.md
- [x] Добавить техническую часть в README.md
- [ ] Поменять расположение best_ensemble_weights, Добавить удаление
      oof_fold.pkl, поменять положение cached_data, submission.csv
- [ ] Убрать скоры в test.csv, разделив на два файла, добавить подсчет
      итогового, финального скора
- [ ] Разделить файлы для скачивания в dvc на train и infer и добавить отдельное
      скачивание в каждой функции
- [ ] Поменять иерархию содержимого папки configs (препроцессинг, претрэйн,
      трэйн, инференс)
- [ ] Убрать все магические константы
- [ ] Переписать документацию, добавить аннотацию типов там, где нет
- [ ] Перепроверить графики MLflow, добавить, если чего-то нет (Также в
      эксперимент записывать использованные гиперпараметры и версию кода (git
      commit id))
- [ ] Добавить пример данных для TensorRT в dvc
- [ ] Запустить на всех данных
- [ ] Написать тесты
